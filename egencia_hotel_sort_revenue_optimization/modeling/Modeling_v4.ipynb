{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import IntegerType,FloatType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3887964"
     ]
    }
   ],
   "source": [
    "# read rate level df with labels\n",
    "search_df = sqlContext.read.parquet('s3://ege-ds-workshops-corp/yixli/data_preparation/bk_rate_all_usd_df_with_label')\n",
    "search_df = search_df.\\\n",
    "            filter(F.col(\"hotel_id\")>0).\\\n",
    "            filter(F.col(\"src_rate_amount_usd\").isNotNull())\n",
    "print(search_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------+--------------+--------+---------+----------+--------------------+------------------+-----------+--------------+--------------------+------------------+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+\n",
      "|          message_id|hotel_id|check_in_date|check_out_date|    tuid|rate_type|rate_index|        message_date|hotel_result_index|hotel_index|bk_hotel_index|             score_1|              city|star_rating|filter_want_in_policy_rates_only|filter_eligible_for_loyalty|filter_free_breakfast|filter_free_wifi|filter_free_parking|eligible_for_loyalty|free_breakfast|free_wifi|free_parking|refundable|src_rate_amount_usd|src_commission_base_usd|src_supply_revenue_usd|label|\n",
      "+--------------------+--------+-------------+--------------+--------+---------+----------+--------------------+------------------+-----------+--------------+--------------------+------------------+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+\n",
      "|0001d378-ba00-459...|   50093|   2019-12-16|    2019-12-17|16766328|     ESRM|         0|2019-12-16 12:03:...|                 3|          3|             3| 0.11723224400446554|     effingham-usa|        2.5|                           false|                      false|                false|           false|              false|               false|          true|     true|        true|     false|   84.1500015258789|     13.460000038146973|     18.59000015258789|    0|\n",
      "|0002094c-7769-47f...| 6248658|   2019-12-19|    2019-12-20|16413906|     ESRA|         4|2019-12-18 05:33:...|                 5|          5|             5|2.999999999992318E12|        rungis-fra|        3.0|                           false|                      false|                false|           false|              false|               false|         false|     true|       false|      true|  82.99244515574613|     13.572699019727597|    14.928857344105351|    0|\n",
      "|0006eaa4-3ec6-416...|16116921|   2019-10-08|    2019-10-09|16972053|     ESRM|         4|2019-10-02 04:13:...|                21|         21|            21| 0.11963478616689523|         paris-fra|        4.0|                           false|                      false|                false|           false|              false|               false|         false|     true|       false|      true| 289.84134567574245|      46.16396200338101|     58.98728489823541|    0|\n",
      "|000c2edf-4ded-4c5...|15921793|   2019-10-10|    2019-10-11|14409585|     ESRA|         3|2019-10-02 02:11:...|                 3|          3|             3|  0.1962500498442387|        london-gbr|        4.0|                           false|                      false|                false|           false|              false|               false|         false|     true|       false|      true|  282.1021642354646|      37.54765795922472|     45.06210899048094|    1|\n",
      "|000d5e93-f25f-469...| 4304608|   2019-12-10|    2019-12-11| 4264220|     ESRA|         0|2019-11-05 00:21:...|                 2|          2|             2|  0.3123233570500529|l'isle-d'abeau-fra|        3.0|                           false|                      false|                false|           false|              false|               false|         false|     true|        true|      true|  94.13067552602395|     12.834994631880221|     14.11960146091831|    1|\n",
      "|001256ad-fe63-4c1...| 4603199|   2019-11-03|    2019-11-04|11494914|     EPRM|         7|2019-10-25 13:07:...|                29|         29|            29|                null|    scottsdale-usa|        4.0|                           false|                      false|                false|           false|              false|               false|         false|     true|        true|      true|              214.0|      38.52000045776367|    51.570000648498535|    0|\n",
      "|00132dee-f3f1-473...|    1213|   2019-11-12|    2019-11-14|11497349|     ESRA|         2|2019-11-06 07:27:...|                 7|          1|             1| 0.37936376618126594|       kolding-dnk|        4.0|                           false|                      false|                false|           false|              false|               false|          true|     true|        true|      true|  155.9884859076216|      22.46536852845574|     28.08208085491133|    0|\n",
      "|00135d85-096b-4e4...| 3547038|   2020-01-16|    2020-01-17|21271111|     EPRA|         1|2019-12-26 11:37:...|                 2|          2|             2| 0.14249051324500944|          oslo-nor|        3.5|                           false|                      false|                false|           false|              false|                true|          true|     true|       false|      true|  133.1224334611786|      20.26225496904283|    22.693455417179443|    0|\n",
      "|00167fb1-e60e-49c...| 8772897|   2019-11-01|    2019-11-02| 4495371|     ESRM|         8|2019-10-09 07:52:...|                 4|          4|             4| 0.14769234547577917|       incheon-kor|        4.5|                           false|                      false|                false|           false|              false|               false|          true|     true|        true|      true| 205.96504094853947|      25.15399120050281|    33.534464697255366|    0|\n",
      "|00170e44-ad13-40a...|   23301|   2019-10-09|    2019-10-11| 7705328|     ESRA|         5|2019-10-02 02:06:...|                 8|          8|             8| 0.13974197051497902|    d√ºsseldorf-deu|        4.0|                           false|                      false|                false|           false|              false|               false|          true|     true|       false|      true| 147.57005962382823|     17.980053361075772|    17.980053361075772|    0|\n",
      "+--------------------+--------+-------------+--------------+--------+---------+----------+--------------------+------------------+-----------+--------------+--------------------+------------------+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "search_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1| 445989|\n",
      "|    0|3441975|\n",
      "+-----+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|eligible_for_loyalty|  count|\n",
      "+--------------------+-------+\n",
      "|                true| 646543|\n",
      "|               false|3241421|\n",
      "+--------------------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"eligible_for_loyalty\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|free_breakfast|  count|\n",
      "+--------------+-------+\n",
      "|          true|1928769|\n",
      "|         false|1959195|\n",
      "+--------------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"free_breakfast\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|free_wifi|  count|\n",
      "+---------+-------+\n",
      "|     true|3573919|\n",
      "|    false| 314045|\n",
      "+---------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"free_wifi\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|free_parking|  count|\n",
      "+------------+-------+\n",
      "|        true|1665273|\n",
      "|       false|2222691|\n",
      "+------------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"free_parking\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|refundable|  count|\n",
      "+----------+-------+\n",
      "|      true|2825518|\n",
      "|     false|1062446|\n",
      "+----------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"refundable\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|rate_type|  count|\n",
      "+---------+-------+\n",
      "|     EPRA| 222840|\n",
      "|     EPRM| 464910|\n",
      "|     ESRM|1876221|\n",
      "|     ESRA| 804932|\n",
      "|      GDS| 519061|\n",
      "+---------+-------+"
     ]
    }
   ],
   "source": [
    "search_df.groupBy(\"rate_type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data_df with features we need\n",
    "data_df = search_df.select('hotel_id', 'check_in_date', 'check_out_date', 'tuid', 'rate_type', 'message_id', 'hotel_result_index', 'rate_index', 'message_date', 'score_1', 'city', 'star_rating', 'filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking', 'eligible_for_loyalty', 'free_breakfast', 'free_wifi', 'free_parking', 'refundable', 'src_rate_amount_usd', 'src_commission_base_usd', 'src_supply_revenue_usd','label')\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3899947"
     ]
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rate number\n",
    "data_df = data_df.withColumn('rate_n',F.count('rate_index').\\\n",
    "                                                  over(Window.partitionBy(\"message_id\",\"hotel_id\",\"check_in_date\",\"check_out_date\",'tuid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column types\n",
    "data_df = data_df.\\\n",
    "             withColumn(\"rate_index\", data_df.rate_index.cast(\"integer\")).\\\n",
    "            withColumn(\"star_rating\", data_df.star_rating.cast(\"float\")).\\\n",
    "            withColumn(\"eligible_for_loyalty\", data_df.eligible_for_loyalty.cast(\"integer\")).\\\n",
    "            withColumn(\"free_breakfast\", data_df.free_breakfast.cast(\"integer\")).\\\n",
    "            withColumn(\"free_wifi\", data_df.free_wifi.cast(\"integer\")).\\\n",
    "            withColumn(\"free_parking\", data_df.free_parking.cast(\"integer\")).\\\n",
    "            withColumn(\"refundable\", data_df.refundable.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+\n",
      "|hotel_id|check_in_date|check_out_date|tuid|rate_type|message_id|hotel_result_index|rate_index|message_date|score_1|city|star_rating|filter_want_in_policy_rates_only|filter_eligible_for_loyalty|filter_free_breakfast|filter_free_wifi|filter_free_parking|eligible_for_loyalty|free_breakfast|free_wifi|free_parking|refundable|src_rate_amount_usd|src_commission_base_usd|src_supply_revenue_usd|label|rate_n|row_index|\n",
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+\n",
      "|       0|            0|             0|   0|        0|         0|                 0|         0|           0| 316071|   0|      16845|                               0|                          0|                    0|               0|                  0|                   0|             0|        0|           0|         0|                  0|                      0|                     0|    0|     0|        0|\n",
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+"
     ]
    }
   ],
   "source": [
    "# check NA\n",
    "data_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in data_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA\n",
    "imputer = Imputer(inputCols=['star_rating'],\n",
    "outputCols=['star_rating'])\n",
    "data_df = imputer.fit(data_df).transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler,StandardScaler\n",
    "\n",
    "categoricalColumns = []\n",
    "#booleanColumns = ['filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking',\n",
    " #                 \"eligible_for_loyalty\",\"free_breakfast\",\"free_wifi\",\"free_parking\",\"refundable\"]\n",
    "booleanColumns = [\"eligible_for_loyalty\",\"free_breakfast\",\"free_wifi\",\"free_parking\",\"refundable\"]\n",
    "numericCols =[\"src_rate_amount_usd\",\"rate_index\",\"star_rating\",'rate_n']\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    \n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + booleanColumns + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"_features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler(inputCol=\"_features\", \n",
    "                            outputCol=\"features\", \n",
    "                            withStd=True, withMean=False)\n",
    "stages += [scaler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Split train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "samples_df = data_df.select(['label','row_index'] + categoricalColumns + booleanColumns + numericCols)\n",
    "\n",
    "pipelineModel = pipeline.fit(samples_df)\n",
    "samples_tm_df = pipelineModel.transform(samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3119666\n",
      "Test Dataset Count: 780281"
     ]
    }
   ],
   "source": [
    "train, test = samples_tm_df.randomSplit([0.8, 0.2], seed = 917)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive smaples\n",
    "train_1= train.where(F.col('label')==1)\n",
    "# downsample negative samples\n",
    "train_0=train.where(F.col('label')==0).sample(False, 0.4, seed = 917)\n",
    "# union together to get new train_df\n",
    "train_final = train_0.union(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1| 359271|\n",
      "|    0|1104352|\n",
      "+-----+-------+"
     ]
    }
   ],
   "source": [
    "train_final.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    0|2760395|\n",
      "|    1| 359271|\n",
      "+-----+-------+"
     ]
    }
   ],
   "source": [
    "train.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    0|690865|\n",
      "|    1| 89416|\n",
      "+-----+------+"
     ]
    }
   ],
   "source": [
    "test.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Modeling and Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# initiate and train model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410485623542459"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|690657|\n",
      "|       1.0|    0|   208|\n",
      "|       0.0|    1| 88931|\n",
      "|       1.0|    1|   485|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([-0.1405, 0.1943, 0.1313, -0.258, 0.1077, -0.5577, -0.8095, 0.0812, -0.1401])"
     ]
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Baseline: Logistic regression with downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# initiate and train model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "lrModel = lr.fit(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7411949428892107"
     ]
    }
   ],
   "source": [
    "# evalution\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|666394|\n",
      "|       1.0|    0| 24471|\n",
      "|       0.0|    1| 72499|\n",
      "|       1.0|    1| 16917|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Random Forest with downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# initiate RF model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "\n",
    "# set parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [4,6])\n",
    "             .addGrid(rf.maxBins, [20,40])\n",
    "             .addGrid(rf.numTrees, [100])\n",
    "             .build())\n",
    "\n",
    "# cross validation to tune parameters\n",
    "def cv_train_model(model,paramGrid,evaluator,numFolds,trainData_new):\n",
    "    cv = CrossValidator(estimator=model, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=numFolds)\n",
    "    cv_Model = cv.fit(trainData_new)\n",
    "    print (cv_Model.bestModel)\n",
    "    return cv_Model\n",
    "\n",
    "# train model\n",
    "rfCvModel = cv_train_model(rf,paramGrid,evaluator,5,train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7561615231337737"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "# evaluate on training data\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderROC\")\n",
    "rf_cv_model_train_prediction = rfCvModel.transform(train_final)\n",
    "evaluator.evaluate(rf_cv_model_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565660886231497"
     ]
    }
   ],
   "source": [
    "# evaluate on test data\n",
    "rf_cv_model_test_prediction = rfCvModel.transform(test)\n",
    "evaluator.evaluate(rf_cv_model_test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|675436|\n",
      "|       1.0|    0| 15429|\n",
      "|       0.0|    1| 73379|\n",
      "|       1.0|    1| 16037|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "rf_cv_model_test_prediction.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o sample_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "labels = sample_predictions[\"label\"]\n",
    "probabilities = sample_predictions[\"probability\"]\n",
    "prob = []\n",
    "for dv in probabilities:\n",
    "    prob.append(dv['values'][1])\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob, pos_label=1);\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig1 = plt.figure(figsize=(7,7))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([-0.01, 1.0]); plt.ylim([-0.01, 1.05]);\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate');\n",
    "plt.title('ROC Curve'); plt.legend(loc=\"lower right\");\n",
    "plt.savefig('image/roc3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "fig2 = plt.figure(figsize=(7,7))\n",
    "#plt.hist(prob, bins=100, alpha=0.5)\n",
    "sample_predictions['prob'] = prob\n",
    "sample_predictions.groupby(\"label\").prob.plot(kind='density', xlim=[-0.01,1.01])\n",
    "#sample_predictions.prob.plot(kind='density', xlim=[-0.01,1.01])\n",
    "plt.title('Distribution of predicted probabilities')\n",
    "plt.xlabel('Predict. prob.'); plt.ylabel('Prob');\n",
    "plt.legend(loc=\"lower right\");\n",
    "plt.xlim(0,1)\n",
    "plt.savefig('image/prob3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.279381\n",
       "1     0.325438\n",
       "2     0.089923\n",
       "3     0.137710\n",
       "4     0.176192\n",
       "5     0.096528\n",
       "6     0.088336\n",
       "7     0.443244\n",
       "8     0.088336\n",
       "9     0.092025\n",
       "10    0.170920\n",
       "11    0.091934\n",
       "12    0.090866\n",
       "13    0.090990\n",
       "14    0.090990\n",
       "15    0.123913\n",
       "16    0.090677\n",
       "17    0.097914\n",
       "18    0.095333\n",
       "19    0.173949\n",
       "20    0.081866\n",
       "21    0.091432\n",
       "22    0.097769\n",
       "23    0.089016\n",
       "24    0.082004\n",
       "25    0.090106\n",
       "26    0.090096\n",
       "27    0.082861\n",
       "28    0.086894\n",
       "29    0.118066\n",
       "30    0.125085\n",
       "31    0.275512\n",
       "32    0.090990\n",
       "33    0.183322\n",
       "34    0.089239\n",
       "35    0.086169\n",
       "36    0.097769\n",
       "37    0.135722\n",
       "38    0.190099\n",
       "39    0.158392\n",
       "40    0.091159\n",
       "41    0.132281\n",
       "42    0.122867\n",
       "43    0.099318\n",
       "44    0.091159\n",
       "45    0.080789\n",
       "46    0.095902\n",
       "47    0.198132\n",
       "48    0.086894\n",
       "49    0.097769\n",
       "Name: prob, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "sample_predictions['prob'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature importance from RF model\n",
    "def extract_feature_importance(featureImp, df, featuresCol):\n",
    "    list_extract = []\n",
    "    # featuresCol: _features\n",
    "    for i in df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "feature_importance = extract_feature_importance(rfCvModel.bestModel.featureImportances, \n",
    "                          rf_cv_model_test_prediction, \"_features\")\n",
    "feature_importance['score'] = feature_importance['score'].round(4)\n",
    "feature_importance['name'] = list(map(lambda x: x.replace('_importantclassVec','').replace('classVec',''),feature_importance['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx                  name   score\n",
      "6    6            rate_index  0.4731\n",
      "8    8                rate_n  0.2618\n",
      "5    5   src_rate_amount_usd  0.1246\n",
      "7    7           star_rating  0.0653\n",
      "0    0  eligible_for_loyalty  0.0268\n",
      "3    3          free_parking  0.0207\n",
      "1    1        free_breakfast  0.0148\n",
      "2    2             free_wifi  0.0096\n",
      "4    4            refundable  0.0032"
     ]
    }
   ],
   "source": [
    "# feature importance from RF model\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|weights             |features            |\n",
      "+--------------------+--------------------+\n",
      "|-0.7580464341332018 |rate_index          |\n",
      "|-0.5986107251283161 |src_rate_amount_usd |\n",
      "|-0.28631360350362706|free_parking        |\n",
      "|-0.1942755570299147 |rate_n              |\n",
      "|-0.19101800670441726|eligible_for_loyalty|\n",
      "|0.14795973395282574 |free_breakfast      |\n",
      "|0.136140129616197   |free_wifi           |\n",
      "|0.09772318452500461 |refundable          |\n",
      "|0.09125523433152447 |star_rating         |\n",
      "+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# extract feature importance from LR model\n",
    "weights = lrModel.coefficients\n",
    "features = feature_importance.sort_values(by=['idx'])['name'].values\n",
    "weightsDF = sqlContext.createDataFrame(sc.parallelize([(float(w),f) for w,f in zip(weights,features)]),['weights','features'])\n",
    "weightsDF = weightsDF.withColumn('weights_abs',F.abs(F.col('weights')))\n",
    "weightsDF = weightsDF.orderBy([\"weights_abs\"], ascending=False)\n",
    "weightsDF = weightsDF.drop(\"weights_abs\")\n",
    "weightsDF.show(len(features),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 read rate level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read rate level df (last search, hotels below PL, both booked and not booked)\n",
    "last_search_df = sqlContext.read.parquet('s3://ege-ds-workshops-corp/yixli/data_preparation/rate_all_usd_2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ger data_df with features we need\n",
    "data_df = last_search_df.select('hotel_id', 'check_in_date', 'check_out_date', 'tuid', 'rate_type', 'message_id', 'hotel_index','hotel_result_index', 'rate_index', 'message_date', 'score_1', 'city', 'star_rating', 'filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking', 'eligible_for_loyalty', 'free_breakfast', 'free_wifi', 'free_parking', 'refundable', 'bk_hotel_index', 'src_rate_amount_usd', 'src_commission_base_usd', 'src_supply_revenue_usd')\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of rates\n",
    "data_df = data_df.withColumn('rate_n',F.count('rate_index').\\\n",
    "                                                  over(Window.partitionBy(\"message_id\",\"hotel_id\",\"check_in_date\",\"check_out_date\",'tuid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column types\n",
    "data_df = data_df.\\\n",
    "             withColumn(\"rate_index\", data_df.rate_index.cast(\"integer\")).\\\n",
    "            withColumn(\"star_rating\", data_df.star_rating.cast(\"float\")).\\\n",
    "            withColumn(\"eligible_for_loyalty\", data_df.eligible_for_loyalty.cast(\"boolean\").cast(\"integer\")).\\\n",
    "            withColumn(\"free_breakfast\", data_df.free_breakfast.cast(\"boolean\").cast(\"integer\")).\\\n",
    "            withColumn(\"free_wifi\", data_df.free_wifi.cast(\"boolean\").cast(\"integer\")).\\\n",
    "            withColumn(\"free_parking\", data_df.free_parking.cast(\"boolean\").cast(\"integer\")).\\\n",
    "            withColumn(\"refundable\", data_df.refundable.cast(\"boolean\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72917266"
     ]
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA\n",
    "imputer = Imputer(inputCols=['star_rating'],\n",
    "outputCols=['star_rating'])\n",
    "data_df = imputer.fit(data_df).transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 pip data into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction of the data_df\n",
    "pipelineModel = pipeline.fit(data_df)\n",
    "data_tm_df = pipelineModel.transform(data_df)\n",
    "prediction=rfCvModel.transform(data_tm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72917266"
     ]
    }
   ],
   "source": [
    "prediction.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract predicted rate-picking up probability from prediction\n",
    "unlist = F.udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "prediction = prediction.withColumn('prob',unlist('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rate revenue\n",
    "prediction = prediction.\\\n",
    "withColumn('rate_revenue',F.col('src_supply_revenue_usd')*F.col('prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction\n",
    "prediction_output = prediction.drop('_features','features','rawPrediction','probability')\n",
    "dir = 's3://ege-ds-workshops-corp/yixli/prediction/'\n",
    "prediction_output.repartition(1).write.mode('overwrite').parquet(dir+'prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
