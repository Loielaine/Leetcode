{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import IntegerType,FloatType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3887964"
     ]
    }
   ],
   "source": [
    "# read rate level df with labels\n",
    "search_df = sqlContext.read.parquet('s3://ege-ds-workshops-corp/yixli/data_preparation/bk_rate_all_usd_df_with_label')\n",
    "search_df = search_df.\\\n",
    "            filter(F.col(\"hotel_id\")>0).\\\n",
    "            filter(F.col(\"src_rate_amount_usd\").isNotNull())\n",
    "print(search_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(search_df,columns):\n",
    "    # create data_df with features we need\n",
    "    data_df = search_df.select(columns)\n",
    "    # add rate number\n",
    "    data_df = data_df.withColumn('rate_n',F.count('rate_index').\\\n",
    "                                over(Window.partitionBy(\"message_id\",\"hotel_id\",\"check_in_date\",\"check_out_date\",'tuid')))  \n",
    "    # convert column types\n",
    "    data_df = data_df.\\\n",
    "                 withColumn(\"rate_index\", data_df.rate_index.cast(\"integer\")).\\\n",
    "                withColumn(\"star_rating\", data_df.star_rating.cast(\"float\")).\\\n",
    "                withColumn(\"eligible_for_loyalty\", data_df.eligible_for_loyalty.cast(\"integer\")).\\\n",
    "                withColumn(\"free_breakfast\", data_df.free_breakfast.cast(\"integer\")).\\\n",
    "                withColumn(\"free_wifi\", data_df.free_wifi.cast(\"integer\")).\\\n",
    "                withColumn(\"free_parking\", data_df.free_parking.cast(\"integer\")).\\\n",
    "                withColumn(\"refundable\", data_df.refundable.cast(\"integer\"))\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hotel_id', 'check_in_date', 'check_out_date', 'tuid', 'rate_type', 'message_id', 'hotel_result_index', 'rate_index', 'message_date', 'score_1', 'city', 'star_rating', 'filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking', 'eligible_for_loyalty', 'free_breakfast', 'free_wifi', 'free_parking', 'refundable', 'src_rate_amount_usd', 'src_commission_base_usd', 'src_supply_revenue_usd','label']\n",
    "data_df = getData(search_df,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+\n",
      "|hotel_id|check_in_date|check_out_date|tuid|rate_type|message_id|hotel_result_index|rate_index|message_date|score_1|city|star_rating|filter_want_in_policy_rates_only|filter_eligible_for_loyalty|filter_free_breakfast|filter_free_wifi|filter_free_parking|eligible_for_loyalty|free_breakfast|free_wifi|free_parking|refundable|src_rate_amount_usd|src_commission_base_usd|src_supply_revenue_usd|label|rate_n|row_index|\n",
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+\n",
      "|       0|            0|             0|   0|        0|         0|                 0|         0|           0| 316071|   0|      16845|                               0|                          0|                    0|               0|                  0|                   0|             0|        0|           0|         0|                  0|                      0|                     0|    0|     0|        0|\n",
      "+--------+-------------+--------------+----+---------+----------+------------------+----------+------------+-------+----+-----------+--------------------------------+---------------------------+---------------------+----------------+-------------------+--------------------+--------------+---------+------------+----------+-------------------+-----------------------+----------------------+-----+------+---------+"
     ]
    }
   ],
   "source": [
    "# check NA\n",
    "data_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in data_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNa(data_df,fillcol):\n",
    "    # fill NA\n",
    "    imputer = Imputer(inputCols=fillcol,\n",
    "    outputCols=fillcol)\n",
    "    data_df = imputer.fit(data_df).transform(data_df)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillcol = ['star_rating']\n",
    "data_df = fillNa(data_df,fillcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler,StandardScaler\n",
    "def transformFeatures(categoricalColumns,booleanColumns,numericCols):\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalColumns:\n",
    "        # Category Indexing with StringIndexer\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        # Add stages.  These are not run here, but will run all at once later on.\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + booleanColumns + numericCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"_features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    # Standardize Features\n",
    "    scaler = StandardScaler(inputCol=\"_features\", \n",
    "                                outputCol=\"features\", \n",
    "                                withStd=True, withMean=False)\n",
    "    stages += [scaler]\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = []\n",
    "#booleanColumns = ['filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking',\n",
    " #                 \"eligible_for_loyalty\",\"free_breakfast\",\"free_wifi\",\"free_parking\",\"refundable\"]\n",
    "booleanColumns = [\"eligible_for_loyalty\",\"free_breakfast\",\"free_wifi\",\"free_parking\",\"refundable\"]\n",
    "numericCols =[\"src_rate_amount_usd\",\"rate_index\",\"star_rating\",'rate_n']\n",
    "stages = transformFeatures(categoricalColumns,booleanColumns,numericCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Split train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "def pipData(stages,data_df)\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    samples_df = data_df.select(['label','row_index'] + categoricalColumns + booleanColumns + numericCols)\n",
    "    pipelineModel = pipeline.fit(samples_df)\n",
    "    samples_tm_df = pipelineModel.transform(samples_df)\n",
    "    return samples_tm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3119666\n",
      "Test Dataset Count: 780281"
     ]
    }
   ],
   "source": [
    "samples_tm_df = pipData(stages,data_df)\n",
    "train, test = samples_tm_df.randomSplit([0.8, 0.2], seed = 917)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampling(df,downSamplingLabel,fraction):\n",
    "    df_keep= df.where(F.col('label')==(1-downSamplingLabel))\n",
    "    df_down=df.where(F.col('label')==downSamplingLabel).sample(False, fraction, seed = 917)\n",
    "    df_final = df_keep.union(df_down)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = downSampling(train,0,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1| 359271|\n",
      "|    0|1104352|\n",
      "+-----+-------+"
     ]
    }
   ],
   "source": [
    "train_final.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Modeling and Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "def fitLogisticRegression(train):\n",
    "    # initiate and train model\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "    lrModel = lr.fit(train)\n",
    "    return lrModel\n",
    "\n",
    "def evaluate(model, test):\n",
    "    # get prediction\n",
    "    predictions = model.transform(test)\n",
    "    # evaluation\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderROC\")\n",
    "    AUC = evaluator.evaluate(predictions)\n",
    "    return predictions, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410485623542459"
     ]
    }
   ],
   "source": [
    "lrModel = fitLogisticRegression(train)\n",
    "predictions, AUC = evaluate(lrModel, test)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|690657|\n",
      "|       1.0|    0|   208|\n",
      "|       0.0|    1| 88931|\n",
      "|       1.0|    1|   485|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseVector([-0.1405, 0.1943, 0.1313, -0.258, 0.1077, -0.5577, -0.8095, 0.0812, -0.1401])"
     ]
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Baseline: Logistic regression with downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410485623542459"
     ]
    }
   ],
   "source": [
    "lrModel = fitLogisticRegression(train_final)\n",
    "predictions, AUC = evaluate(lrModel, test)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|666394|\n",
      "|       1.0|    0| 24471|\n",
      "|       0.0|    1| 72499|\n",
      "|       1.0|    1| 16917|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Random Forest with downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# initiate RF model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "\n",
    "# set parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [4,6])\n",
    "             .addGrid(rf.maxBins, [20,40])\n",
    "             .addGrid(rf.numTrees, [100])\n",
    "             .build())\n",
    "\n",
    "# cross validation to tune parameters\n",
    "def cv_train_model(model,paramGrid,evaluator,numFolds,trainData_new):\n",
    "    cv = CrossValidator(estimator=model, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=numFolds)\n",
    "    cv_Model = cv.fit(trainData_new)\n",
    "    print (cv_Model.bestModel)\n",
    "    return cv_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7565660886231497"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "rfCvModel = cv_train_model(rf,paramGrid,evaluator,5,train_final)\n",
    "# evaluate on test data\n",
    "rf_cv_model_test_prediction, AUC = evaluate(rfCvModel,test)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|prediction|label| count|\n",
      "+----------+-----+------+\n",
      "|       0.0|    0|675436|\n",
      "|       1.0|    0| 15429|\n",
      "|       0.0|    1| 73379|\n",
      "|       1.0|    1| 16037|\n",
      "+----------+-----+------+"
     ]
    }
   ],
   "source": [
    "rf_cv_model_test_prediction.groupBy('prediction','label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions = rf_cv_model_test_prediction.sample(False,0.1, seed = 917)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o sample_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "labels = sample_predictions[\"label\"]\n",
    "probabilities = sample_predictions[\"probability\"]\n",
    "prob = []\n",
    "for dv in probabilities:\n",
    "    prob.append(dv['values'][1])\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob, pos_label=1);\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig1 = plt.figure(figsize=(7,7))\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([-0.01, 1.0]); plt.ylim([-0.01, 1.05]);\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate');\n",
    "plt.title('ROC Curve'); plt.legend(loc=\"lower right\");\n",
    "plt.savefig('image/roc3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "fig2 = plt.figure(figsize=(7,7))\n",
    "#plt.hist(prob, bins=100, alpha=0.5)\n",
    "sample_predictions['prob'] = prob\n",
    "sample_predictions.groupby(\"label\").prob.plot(kind='density', xlim=[-0.01,1.01])\n",
    "#sample_predictions.prob.plot(kind='density', xlim=[-0.01,1.01])\n",
    "plt.title('Distribution of predicted probabilities')\n",
    "plt.xlabel('Predict. prob.'); plt.ylabel('Prob');\n",
    "plt.legend(loc=\"lower right\");\n",
    "plt.xlim(0,1)\n",
    "plt.savefig('image/prob3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature importance from RF model\n",
    "def extractFeatureImportance(featureImp, df, featuresCol):\n",
    "    list_extract = []\n",
    "    # featuresCol: _features\n",
    "    for i in df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    feature_importance = varlist.sort_values('score', ascending = False)\n",
    "    feature_importance['score'] = feature_importance['score'].round(4)\n",
    "    feature_importance['name'] = list(map(lambda x: x.replace('_importantclassVec','').replace('classVec',''),feature_importance['name']))\n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx                  name   score\n",
      "6    6            rate_index  0.4731\n",
      "8    8                rate_n  0.2618\n",
      "5    5   src_rate_amount_usd  0.1246\n",
      "7    7           star_rating  0.0653\n",
      "0    0  eligible_for_loyalty  0.0268\n",
      "3    3          free_parking  0.0207\n",
      "1    1        free_breakfast  0.0148\n",
      "2    2             free_wifi  0.0096\n",
      "4    4            refundable  0.0032"
     ]
    }
   ],
   "source": [
    "# feature importance from RF model\n",
    "feature_importance = extractFeatureImportance(rfCvModel.bestModel.featureImportances, \n",
    "                              rf_cv_model_test_prediction, \"_features\")\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWeights(model,feature_importance):\n",
    "    weights = model.coefficients\n",
    "    features = feature_importance.sort_values(by=['idx'])['name'].values\n",
    "    weightsDF = sqlContext.createDataFrame(sc.parallelize([(float(w),f) for w,f in zip(weights,features)]),['weights','features'])\n",
    "    weightsDF = weightsDF.withColumn('weights_abs',F.abs(F.col('weights')))\n",
    "    weightsDF = weightsDF.orderBy([\"weights_abs\"], ascending=False)\n",
    "    weightsDF = weightsDF.drop(\"weights_abs\")\n",
    "    return weightsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|weights             |features            |\n",
      "+--------------------+--------------------+\n",
      "|-0.7580464341332018 |rate_index          |\n",
      "|-0.5986107251283161 |src_rate_amount_usd |\n",
      "|-0.28631360350362706|free_parking        |\n",
      "|-0.1942755570299147 |rate_n              |\n",
      "|-0.19101800670441726|eligible_for_loyalty|\n",
      "|0.14795973395282574 |free_breakfast      |\n",
      "|0.136140129616197   |free_wifi           |\n",
      "|0.09772318452500461 |refundable          |\n",
      "|0.09125523433152447 |star_rating         |\n",
      "+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# extract weights from LR model\n",
    "weightsDF = extractWeights(lrModel,feature_importance)\n",
    "weightsDF.show(len(features),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 read rate level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read rate level df (last search, hotels below PL, both booked and not booked)\n",
    "last_search_df = sqlContext.read.parquet('s3://ege-ds-workshops-corp/yixli/data_preparation/rate_all_usd_2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data_df\n",
    "columns = ['hotel_id', 'check_in_date', 'check_out_date', 'tuid', 'rate_type', 'message_id', 'hotel_result_index', 'rate_index', 'message_date', 'score_1', 'city', 'star_rating', 'filter_want_in_policy_rates_only', 'filter_eligible_for_loyalty', 'filter_free_breakfast', 'filter_free_wifi', 'filter_free_parking', 'eligible_for_loyalty', 'free_breakfast', 'free_wifi', 'free_parking', 'refundable', 'src_rate_amount_usd', 'src_commission_base_usd', 'src_supply_revenue_usd','label']\n",
    "data_df = getData(last_search_df,columns)\n",
    "# Fill NA\n",
    "fillcol = ['star_rating']\n",
    "data_df = fillNa(data_df,fillcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 pip data into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictData(model,data_df):\n",
    "    # get prediction of the data_df\n",
    "    pipelineModel = pipeline.fit(data_df)\n",
    "    data_tm_df = pipelineModel.transform(data_df)\n",
    "    prediction=model.transform(data_tm_df)\n",
    "    # extract predicted rate-picking up probability from prediction\n",
    "    unlist = F.udf(lambda x: float(list(x)[0]), DoubleType())\n",
    "    prediction = prediction.withColumn('prob',unlist('probability'))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictData(rfCvModel,data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rate revenue\n",
    "prediction = prediction.\\\n",
    "withColumn('rate_revenue',F.col('src_supply_revenue_usd')*F.col('prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction\n",
    "prediction_output = prediction.drop('_features','features','rawPrediction','probability')\n",
    "dir = 's3://ege-ds-workshops-corp/yixli/prediction/'\n",
    "prediction_output.repartition(1).write.mode('overwrite').parquet(dir+'prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
